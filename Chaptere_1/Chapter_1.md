# 统计学习方法（第二版）第一章学习笔记
- [统计学习方法（第二版）第一章学习笔记](#统计学习方法第二版第一章学习笔记)
  - [明确目的](#明确目的)
  - [极大似然估计的假设前提](#极大似然估计的假设前提)
  - [贝叶斯估计的假设前提](#贝叶斯估计的假设前提)
  - [先验概率、后验概率](#先验概率后验概率)
  - [似然](#似然)

##	废话在前
&ensp;&ensp; 本文的名字虽叫学习笔记，但是并不是记录统计学习方法的详细笔记，而只是对一些难以理解的知识点提出自己的一些尚不成熟的看法。

&ensp;&ensp; 第一章主要是一些基本概念，第一章的两道习题都是关于贝叶斯估计和极大似然估计的，在读到这两个估计方法的时候（包括之前学这些东西的时候）一直没搞懂这两个方法在干什么。经过了看别人的博客等，结合一些自己的看法，说明一下（这就意味着很可能有错误的理解）。 好了，下面请容许我一本正经的胡说八道。

## 明确目的
&ensp;&ensp; 过多的数学可能让我们脑袋昏昏的，搞得不知所措，那就先放下数学，想一想我们究竟在干什么（也即我们的目的）？

&ensp;&ensp; 无论是什么方法，我们的目的其实都是要获得一个模型，也就是$\theta$ 这个模型参数，而我们的已知条件是数据$D$。 也就是说我们要找出在数据$D$出现的情况下，最可能的$\theta$。

&ensp;&ensp; 上面这个就是我们朴素的愿望。换成数学语言就是，我们希望找到一个$\theta$使得$P(\theta|D)$最大，这个概率值在数学中叫做后验概率，所以我们的**目的**就是：**最大化这个后验概率**。

&ensp;&ensp;  同时，根据贝叶斯定理有：
$$
P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}
$$

## 极大似然估计的假设前提
&ensp;&ensp; 极大似然估计和贝叶斯估计是基于两个不同的假设得到的方法，这里先说明极大似然估计的假设前提。 极大似然估计认为模型参数$\theta$是客观存在的，只是我们太菜了，算不出来它到底是多少。 但是，我们太菜关模型什么事呢？ 这意味着$\theta$是确定的，哪怕实际上我们需要观察无限多的样本才能确定它的值。 既然模型参数是客观存在的，那么$P(\theta)$就是个常数。 同时，注意到数据的分布$P(D)$也是个常数。 这时，最大化后验概率的目标就可以简化为最大化$P(D|\theta)$，而这个$P(D|\theta)$在数学上就叫做似然函数，因此这个方法就叫做极大似然估计。

&ensp;&ensp; 极大似然估计是典型的频率学派。 啥是个频率学派呢？ 可以理解为，他们认为出现的事件就是大概率事件，我们经过大规模的观测样本后，样本的频率就是概率本身。 极大似然估计就是认为我们观测到这个样本，比如数据$D$，那么说明数据$D$出现的概率是最大的，只需要最大化$D$出现的概率就能得到模型参数，也即最大化$P(D|\theta)$。

## 贝叶斯估计的假设前提
&ensp;&ensp; 贝叶斯学派与频率学派不同，贝叶斯估计认为$\theta$并不是一个客观存在，它本身也是一个随机的（也就是说，模型本身也具有随机性，不存在一个确定的模型）。 贝叶斯学派认为菜的不是我们，而是模型他自己本身就是随机的，那怎么能说我太菜算不出来呢？ 既然$\theta$是随机的，那么我们只能老老实实地根据观测样本来估计参数了。

&ensp;&ensp; 好了，还记得我们的目的么？ 最大化后验概率（暂时只讨论最大化后验概率的思想）！ 由于$P(D)$是常数，而$P(\theta)$和$P(D|\theta)$都不是常数，我们最大化后验概率的目标将转化为最大化$P(\theta)P(D|\theta)$。  由于贝叶斯估计假设$\theta$也是随机变量，我们在构建模型时，首先就要假设出$\theta$的分布。

## 先验概率、后验概率
&ensp;&ensp; 学习概率论的时候，就很头疼，什么先验概率、后验概率、似然函数... 根本就记不住啊。 

&ensp;&ensp; 我通俗的理解是，在条件概率中，存在一定的因果关系。 具体到上面的一些表述。 $\theta$就是“因”，而$D$就是“果”。也就是说，是因为有了$\theta$这么一个模型参数，才有了$P_{\theta}(Y|X)$这么个映射关系，我们才能观测到具体的$D$这组数据（就是$X$和$Y$啦）。 因和果，因在前，果在后。 所以，$P(\theta)$叫做先验概率，而$P(\theta|D)$叫做后验概率（“果”变成了条件概率中的条件）。

## 似然
&ensp;&ensp; 似然的英文是likelihood，而概率的英文是probability。 他们其实有着紧密的联系，而又有所不同。 $P(D|\theta)$既可以看作是似然函数也可以看作是一个条件概率。 我们知道，$P(D|\theta)$是关于$D$和$\theta$的一个函数。 如果我们把$\theta$看作一个常量，那么$P(D|\theta)$就是$D$的概率函数（$D$是数据，如果你觉得别扭，可以换成$X$）。 如果我们把$D$看作常量，那么$P(D|\theta)$就是一个关于$\theta$的函数，这个函数就是似然函。